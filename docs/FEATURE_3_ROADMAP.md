# Feature 3 Roadmap — Multi‑Modal Content Analysis

Status: Proposal
Owner: ML / Platform Team
Estimated effort: 4–6 weeks (MVP), 8–10 weeks (production)

## Executive summary

Feature 3 extends the News Topic Intelligence platform with multi‑modal analysis: automatically ingest and analyze images accompanying articles (or images generated by the platform), fuse visual features with text, and use the joint signal to improve classification, summarization, trend detection, and content‑conditioned image quality checks.

Business value:

- Improve classification accuracy and robustness for articles with images (and when images convey important context).
- Reduce false positives/negatives where visual context disambiguates text (e.g., memes, product photos).
- Enable new UX: image‑aware summaries, image + text explanations, and visual search/trend insights.

Success metrics (MVP):

- +2–5% absolute F1 on categories where images carry signal (measured on a labeled holdout)
- End‑to‑end inference latency (text+image) p50 < 350 ms (CPU + single GPU for images) for single requests
- Integration tests and CI coverage for major inference paths

---

## Scope (MVP vs Phases)

MVP (4–6 weeks):

- Ingest images for articles (URL or uploaded) and extract visual embeddings (CLIP/VisionTransformer)
- Implement text + image late fusion classifier (concatenated embeddings → classifier head)
- Add API endpoint to submit article with optional `image_url` and return fused prediction
- Streamlit UI: allow uploading or pointing to an image, show fused prediction and confidence
- Unit/integration tests and MLflow experiment tracking for multi‑modal experiments

Phase 2 (production hardening):

- Early fusion / attention‑based cross‑modal models (e.g., ViLT, CLIP‑adapter) for stronger signal
- Image quality & safety checks, image OCR, EXIF parsing, and NSFW filtering
- Dataset augmentation and labeling pipeline for image-text pairs (DVC flows)
- Monitoring: per‑modal drift detection, image feature distribution monitoring, and alerts
- A/B test multimodal vs text‑only models with live traffic

Phase 3 (advanced):

- Multi‑modal summarization and visual prompt generation for image editing/inpainting
- Visual explainability (GradCAM, attention maps) integrated in Streamlit
- Indexing & search over visual embeddings for trend detection and visual similarity

---

## Data & labeling

Data sources:

- Existing datasets: HuffPost (images included), public news image corpora
- Crawl/harvest images for existing article records in `data/raw/*`
- Human labeling: sample 2k–5k image+article pairs for validation and targeted categories

Data pipeline:

- Extend `scripts/seed_db.py` / `scripts/manage.py` to capture `image_url` or uploaded images
- Store raw images under `data/images/raw/` tracked by DVC
- Extract cached embeddings to `data/images/embeddings/` and track with DVC
- Add labeler UI for quick human verification (small Streamlit tool)

Privacy & safety:

- Strip PII from images (OCR redaction if necessary)
- Respect robots.txt and license when harvesting external images
- Rate limit and cache remote fetches

---

## Model design options

Baseline (MVP):

- Text encoder: existing transformer (DistilBART/Transformer used in repo)
- Image encoder: CLIP ViT‑B/32 (from Hugging Face / OpenAI CLIP) -> 512/768‑dim embedding
- Fusion: concat(text_emb, image_emb) → small MLP classifier (2–3 layers) → softmax
- Training: freeze encoders initially, fine‑tune classifier head; later unfreeze top layers

Stronger alternatives (Phase 2):

- CLIP‑adapter: lightweight adapter layers to adapt CLIP for classification
- Cross‑modal transformer (ViLT / FLAVA style) for early fusion and attention
- Multi‑task head that jointly predicts category + image quality + applicability score

Evaluation:

- Standard classification metrics (precision/recall/F1, macro and per‑class)
- Ablation: text‑only vs image‑only vs fused
- Latency and resource profiling

---

## API changes & UX

API (MVP additions):

- POST `/classify_news` (existing) extended to accept `image_url` or `image_base64`
- New endpoint `POST /images/process` (internal) to fetch, validate, and embed image
- Response includes `modalities: ['text','image']`, `fusion_used: true/false`, and `confidence` per modality

Streamlit UI:

- Update Images tab to submit image alongside article text
- Visualize per‑modal contributions (e.g., text confidence vs image confidence)
- Provide toggles to run text‑only vs multi‑modal inference for comparison

Backward compatibility:

- If `image_url` absent, service falls back to text‑only pipeline
- Keep response schema backward compatible (add new optional keys)

---

## Infra & operational considerations

Compute:

- GPU for image embedding (can be small—CLIP runs on CPU but faster on GPU); recommend single GPU node for embeddings + image generation
- Storage for image assets (S3 or `data/images/` local for dev)

Serving options:

- Option A (simple): embed images at ingestion and store embeddings in DB; serve classifier on CPU using cached embeddings
- Option B (real‑time): run image encoder at inference time on GPU and run fused model per request
- Hybrid recommended: precompute embeddings for known images; support real‑time for user uploads

Monitoring:

- Add image feature distribution metrics to `/metrics` (Prometheus): mean embedding norms, per-class image counts, failed fetches
- Drift detection for image embedding distribution (Evidently or custom) and alerting

Costs & quotas:

- DVC storage increase, S3 costs if used, GPU hours for training/fine‑tuning
- Add `MAX_IMAGE_SIZE_MB` and `DAILY_IMAGE_QUOTA` env vars

---

## Milestones & timeline (4–6 weeks MVP)

Week 0 (planning): finalize dataset, model choice, and minimal API contract (2–3 days)

Week 1 (data & infra):

- Extend data model and DVC dataset for image ingestion
- Implement image fetcher and caching layer
- Add tests for image fetching/parsing

Week 2 (models & experiments):

- Implement CLIP embedding extraction pipeline and MLflow experiment
- Train concat fusion classifier head; run baseline eval (text-only vs fused)

Week 3 (API & UI):

- Extend `/classify_news` to accept images and wire to fusion model
- Update Streamlit Images tab for upload + visualization
- Add unit + integration tests

Week 4 (hardening & docs):

- Performance profiling and optimization (batching, caching)
- Add monitoring metrics, observability dashboards
- Document in `docs/FEATURE_3_ROADMAP.md` and `docs/IMAGE_GENERATION.md` cross‑links

Acceptance criteria (MVP):

- Fusion model runs end‑to‑end on sample data and improves target metric on holdout
- API supports `image_url` and returns fused prediction with modality info
- Streamlit UI demonstrates upload and fused prediction
- CI passes unit + integration tests for new paths

---

## Tests & validation

Unit tests:

- Image fetcher edge cases (404s, timeouts, large files)
- Embedding extraction deterministic shape/ dtype tests

Integration tests:

- Full classify endpoint with and without images
- Streamlit UI smoke tests (local)

Evaluation suite:

- Holdout labeled set for multi‑modal categories
- A/B testing harness to compare text‑only vs multi‑modal in prod traffic

Robustness:

- Adversarial tests: image perturbations, occlusions, swapped images
- Safety tests: NSFW filtering, corrupted images handling

---

## CI / MLOps changes

- MLflow experiments for multi‑modal runs and model registry entries
- DVC pipeline stages to download, preprocess, and extract embeddings
- GitHub Actions: add workflow to run key multi‑modal unit tests and basic integration smoke tests
- Dockerfiles: include image encoder dependencies (torch, torchvision, huggingface/transformers, timm)

---

## Risk & mitigations

Risk: image harvesting legal / licensing issues

- Mitigation: only use license‑approved sources; for web scrape, store metadata and license info

Risk: inference latency and GPU bottleneck

- Mitigation: precompute embeddings, use caching, horizontal scale GPU nodes or batch processing

Risk: NSFW or unsafe images

- Mitigation: run lightweight NSFW filter at ingestion, allow manual review queue for flagged images

Risk: dataset imbalance (few images for certain classes)

- Mitigation: targeted labeling campaigns and synthetic augmentation (cropping, color jitter)

---

## Resource estimate

MVP: 1 ML engineer + 1 infra/DevOps part‑time, 4–6 weeks
Production: +1 engineer for hardening + QA, 6–10 additional weeks
GPU: one 24–48 hour fine‑tuning slot on an A100/RTX 4090 or repeated shorter runs on RTX 4060
Storage: +5–20 GB for images/embeddings depending on scope

---

## Deliverables (MVP)

- `docs/FEATURE_3_ROADMAP.md` (this file)
- DVC dataset with raw images and cached embeddings
- CLIP embedding extraction script (`app/services/image_encoder.py`)
- Fusion classifier training script + MLflow tracking
- API extensions (image ingestion + fused classify endpoint)
- Streamlit UI changes to support image uploads and visualization
- CI tests and documentation

---

## Next steps (today)

1. Approve roadmap and pick MVP model (CLIP baseline recommended)
2. Create a small priority dataset (1k–2k image+article pairs) and track with DVC
3. Implement the image fetcher + embedding extractor and add unit tests
4. Run baseline fused experiments and report back with metrics

_End of roadmap_
